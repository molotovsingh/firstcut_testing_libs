{
  "order_id": "performance-timing-001",
  "priority": "high",
  "supercontext": {
    "repository": "docling_langextract_testing",
    "mission": "Instrument the legal events pipeline so Docling parsing and LLM extraction durations are captured, logged, and surfaced in exports for performance analysis."
  },
  "goal": "Record per-document timing metrics (Docling vs. Event Extractor) during Streamlit and CLI runs, propagate them to logs and output artifacts, and document how to read the new metrics.",
  "execution_instructions": [
    "Read all tasks and steps before writing code.",
    "Implement the instrumentation in a way that leaves the existing five-column contract untouched.",
    "Review the design mantras in CLAUDE.md (start small, ship value, prefer simple patterns) before planning instrumentation changes.",
    "If any step is blocked (missing dependency, unclear interface), pause and escalate with notes rather than guessing."
  ],
  "tasks": [
    {
      "id": "design-metrics-structure",
      "description": "Define how timing data is stored per document and per event record.",
      "steps": [
        "Add or update a lightweight structure (e.g., dataclass or dict) to hold `docling_seconds`, `event_extractor_seconds`, and `total_seconds` for each processed document.",
        "Extend `EventRecord.attributes` (or an equivalent metadata field) to include timing information without breaking consumers.",
        "Plan how the metrics will appear in exports (JSON, CSV, XLSX) and confirm the layout with existing column requirements."
      ]
    },
    {
      "id": "instrument-pipeline",
      "description": "Capture timings inside the legal events pipeline.",
      "steps": [
        "Wrap the Docling parsing call in `LegalEventsPipeline._process_single_file_guaranteed` (or the relevant helper) with high-resolution timers (e.g., `time.perf_counter`).",
        "Wrap the event extraction call with another timer and compute total duration.",
        "Store the captured timings in the structure designed in Task `design-metrics-structure` and attach them to the resulting records.",
        "Emit structured log statements summarizing timings per document (include document name, docling_ms, extractor_ms, total_ms)."
      ]
    },
    {
      "id": "surface-metrics",
      "description": "Expose timing data to users and benchmark artifacts.",
      "steps": [
        "Update the export logic (CSV/JSON/XLSX writers and Streamlit download) so each exported row includes timing metadata, either as dedicated columns or nested JSON, while keeping the original five columns intact.",
        "Ensure CLI or automated runs persist the timing metrics alongside existing outputs (e.g., add fields to `output/...json`).",
        "Reference relevant guardrails (security, configuration, design mantras) so executors know what to consult before modifying exports or docs.",
        "Document how to interpret the new metrics and where to find them (README.md Performance/Benchmark section or a new doc entry)."
      ]
    },
    {
      "id": "qa-and-validation",
      "description": "Verify instrumentation accuracy and document the outcome.",
      "steps": [
        "Run the Streamlit app with at least two sample documents and confirm timings appear in logs and exports.",
        "Run the CLI (`uv run python src/main.py`) against the same documents and verify the recorded timings match expectations.",
        "Add a short note to `docs/reports/` summarizing the measurement results (include document names, timings observed, and any anomalies)."
      ]
    }
  ],
  "acceptance_criteria": [
    "Each processed document records docling_seconds, event_extractor_seconds, and total_seconds, and these values are logged.",
    "Timing metadata is visible in exported JSON/CSV/XLSX without breaking the five-column legal events table.",
    "README (or a referenced doc) explains where to find and how to read the new performance metrics.",
    "A report in docs/reports documents the validation run with real timing numbers.",
    "Tests/CLI runs complete successfully with the new instrumentation enabled.",
    "Order references the repository guardrails (security, configuration, design mantras) and executors acknowledge them in the final summary."
  ],
  "constraints": {
    "what_not_to_do": [
      "Do not alter the existing five-column schema; timing data must be additive.",
      "Do not introduce heavy profiling libraries\u2014use standard timing utilities.",
      "Do not suppress existing logging or error handling paths while adding timers.",
      "Do not commit large benchmark datasets; keep validation artifacts lightweight."
    ],
    "escalation_guidance": "If timing cannot be safely captured for a provider (e.g., missing hooks), log the gap and escalate before shipping partial metrics."
  },
  "version": "v1.0"
}