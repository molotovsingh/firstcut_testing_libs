{
  "order_id": "performance-timing-001-revised",
  "revision": "v1.1",
  "priority": "high",
  "changelog": [
    "Fixed: Added explicit attributes preservation through export chain",
    "Fixed: Clarified per-document timing (all events from same doc share timing)",
    "Fixed: Removed legacy CLI validation (replaced with Streamlit-only validation)",
    "Added: Concrete export format examples with timing columns",
    "Added: Optional ENABLE_PERFORMANCE_TIMING toggle for production control",
    "Added: Error handling guidance for partial timing failures",
    "Added: Timing metadata structure specification"
  ],
  "supercontext": {
    "repository": "docling_langextract_testing",
    "mission": "Instrument the legal events pipeline to capture and surface per-document timing metrics (Docling parsing vs LLM extraction) for performance analysis and optimization.",
    "architecture_note": "Current pipeline converts EventRecord → dict losing attributes. Must preserve timing data through export chain."
  },
  "goal": "Record per-document timing metrics (Docling parsing, event extraction, total) during Streamlit runs, preserve them through the export pipeline, and surface in logs/exports without breaking the five-column legal events contract.",
  "execution_instructions": [
    "Read all tasks and steps before writing code.",
    "Review CLAUDE.md mantras (start small, ship value, prefer simple patterns) before implementation.",
    "CRITICAL: Ensure EventRecord.attributes timing data survives dict conversion in _process_single_file_guaranteed (line 226-233).",
    "Use time.perf_counter() for high-resolution timing (not time.time()).",
    "If any step is blocked (missing dependency, unclear interface), pause and escalate with notes rather than guessing."
  ],
  "tasks": [
    {
      "id": "design-timing-structure",
      "description": "Define timing data structure and preservation strategy.",
      "steps": [
        "Add @dataclass TimingMetrics to src/core/interfaces.py with fields: docling_seconds (float), extractor_seconds (float), total_seconds (float), document_name (str).",
        "Update EventRecord to include optional timing: TimingMetrics field or add to attributes dict with key 'performance_timing'.",
        "CRITICAL FIX: Modify legal_pipeline_refactored.py:226-233 dict conversion to preserve attributes OR add timing as separate dict fields.",
        "Design export format: Add optional timing columns to CSV/XLSX (Docling_Seconds, Extractor_Seconds, Total_Seconds) and nested timing object to JSON.",
        "Plan toggle: Add ENABLE_PERFORMANCE_TIMING env var (default: true for dev, false for production)."
      ]
    },
    {
      "id": "instrument-pipeline",
      "description": "Capture timings in legal events pipeline with error handling.",
      "steps": [
        "Wrap docling extraction (line 205: self.document_extractor.extract) with time.perf_counter() before/after.",
        "Wrap event extraction (line 222: self.event_extractor.extract_events) with time.perf_counter() before/after.",
        "Calculate total_seconds = docling_seconds + extractor_seconds.",
        "CLARIFICATION: Store timing at document level (all events from same document get identical timing values).",
        "Attach timing to EventRecord.attributes['performance_timing'] = {'docling_s': X, 'extractor_s': Y, 'total_s': Z}.",
        "Add error handling: If timing capture fails (e.g., exception during extract), log warning and set timing to None/null.",
        "Emit structured log: logger.info(f'⏱️ {doc_name}: Docling={docling_s:.3f}s, Extractor={extractor_s:.3f}s, Total={total_s:.3f}s')."
      ]
    },
    {
      "id": "preserve-through-export",
      "description": "Fix dict conversion to preserve timing and update export logic.",
      "steps": [
        "CRITICAL: Fix _process_single_file_guaranteed dict conversion (line 226-233) to include timing from attributes.",
        "Option A: Add timing fields directly to dict: {'number': ..., 'docling_seconds': ..., 'extractor_seconds': ..., 'total_seconds': ...}.",
        "Option B: Preserve full attributes dict: {'number': ..., 'attributes': event_record.attributes}.",
        "Update TableFormatter.prepare_for_export() to handle timing columns if present (CSV/XLSX get extra columns, JSON gets nested timing object).",
        "Ensure five-column headers remain unchanged (timing columns are additive, positioned after Document Reference).",
        "Add format examples to exports: CSV gets Docling_Seconds,Extractor_Seconds,Total_Seconds columns; JSON gets 'timing': {'docling_s': 1.23, ...} object."
      ]
    },
    {
      "id": "surface-metrics",
      "description": "Surface timing in Streamlit UI and export downloads.",
      "steps": [
        "Update streamlit_common.py display_legal_events_table() to show timing summary metrics (avg Docling time, avg Extractor time, avg Total time) if timing data present.",
        "Update create_download_section() to ensure timing columns/objects appear in all three export formats (XLSX, CSV, JSON).",
        "Add timing toggle check: Only include timing in exports if ENABLE_PERFORMANCE_TIMING=true in .env.",
        "Add documentation to README.md under new '## Performance Metrics' section explaining timing columns and how to enable/disable."
      ]
    },
    {
      "id": "validation-and-reporting",
      "description": "Validate instrumentation with real documents and document results.",
      "steps": [
        "Run Streamlit app (uv run streamlit run app.py) with ENABLE_PERFORMANCE_TIMING=true.",
        "Upload 2 test documents: sample_pdf/famas_dispute/Answer to Request for Arbitration.pdf (small, ~15 pages) and sample_pdf/amrapali_case/Amrapali Allotment Letter.pdf (larger).",
        "Verify timing appears in console logs with format: ⏱️ {doc_name}: Docling=X.XXXs, Extractor=X.XXXs, Total=X.XXXs.",
        "Download all 3 export formats (XLSX, CSV, JSON) and verify timing columns/objects present.",
        "Create validation report: docs/reports/performance-timing-validation.md with: document names, observed timings, export format examples, any anomalies.",
        "NOTE: Legacy CLI (src/main.py) is NOT used - validation is Streamlit-only.",
        "Optional: Add timing assertions to existing tests (if timing is enabled, check that values are > 0)."
      ]
    }
  ],
  "acceptance_criteria": [
    "Each processed document records docling_seconds, extractor_seconds, total_seconds, and these values appear in structured logs.",
    "All events from the same document share identical timing values (document-level timing, not per-event).",
    "Timing metadata survives dict conversion in _process_single_file_guaranteed and reaches export functions.",
    "CSV/XLSX exports include optional timing columns (Docling_Seconds, Extractor_Seconds, Total_Seconds) after Document Reference column.",
    "JSON exports include nested timing object: {'timing': {'docling_s': 1.23, 'extractor_s': 2.34, 'total_s': 3.57}}.",
    "Original five-column schema remains unchanged (No, Date, Event Particulars, Citation, Document Reference).",
    "README.md documents the new performance metrics (where to find, how to enable/disable via ENABLE_PERFORMANCE_TIMING).",
    "Validation report (docs/reports/performance-timing-validation.md) includes real timing numbers from 2+ test documents.",
    "Streamlit UI optionally displays timing summary metrics (average times) if timing data is present.",
    "Implementation respects CLAUDE.md mantras: minimal scope, standard library timing, additive changes only."
  ],
  "export_format_examples": {
    "csv_with_timing": "No,Date,Event Particulars,Citation,Document Reference,Docling_Seconds,Extractor_Seconds,Total_Seconds\n1,2024-09-21,Lease agreement entered,RTA 2010,lease.pdf,1.234,2.567,3.801\n2,2024-10-01,Security deposit paid,RTA 2010,lease.pdf,1.234,2.567,3.801",
    "json_with_timing": {
      "events": [
        {
          "number": 1,
          "date": "2024-09-21",
          "event_particulars": "Lease agreement entered",
          "citation": "RTA 2010",
          "document_reference": "lease.pdf",
          "timing": {
            "docling_s": 1.234,
            "extractor_s": 2.567,
            "total_s": 3.801
          }
        }
      ]
    },
    "xlsx_with_timing": "Same as CSV format with additional columns after Document Reference"
  },
  "constraints": {
    "what_not_to_do": [
      "Do not alter the existing five-column schema (No, Date, Event Particulars, Citation, Document Reference).",
      "Do not introduce heavy profiling libraries—use time.perf_counter() from standard library.",
      "Do not suppress existing logging or error handling paths while adding timers.",
      "Do not commit large benchmark datasets; keep validation artifacts lightweight (<100KB).",
      "Do not assume CLI (src/main.py) is functional—validation is Streamlit-only."
    ],
    "escalation_guidance": "If timing cannot be safely captured for a provider (e.g., adapter doesn't expose timing hooks), log the gap with provider name and escalate before shipping partial metrics.",
    "rollback_safety": "ENABLE_PERFORMANCE_TIMING toggle allows disabling timing in production without code changes."
  },
  "technical_notes": {
    "critical_code_locations": {
      "timing_insertion_point_1": "src/core/legal_pipeline_refactored.py:205 (Docling extraction)",
      "timing_insertion_point_2": "src/core/legal_pipeline_refactored.py:222 (Event extraction)",
      "dict_conversion_bug": "src/core/legal_pipeline_refactored.py:226-233 (attributes lost here - MUST FIX)",
      "export_logic": "src/core/table_formatter.py:137 (prepare_for_export function)",
      "interfaces": "src/core/interfaces.py:20-28 (EventRecord dataclass)"
    },
    "timing_precision": "Use time.perf_counter() for high-resolution monotonic timing, format to 3 decimal places (millisecond precision).",
    "per_document_timing": "All events extracted from a single document share the same timing values (Docling runs once per doc, not per event).",
    "toggle_behavior": "If ENABLE_PERFORMANCE_TIMING=false, timing capture is skipped and timing columns/objects are omitted from exports."
  },
  "version": "v1.1",
  "revised_by": "Claude Code",
  "revision_date": "2025-10-02"
}
